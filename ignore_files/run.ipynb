{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, TimeDistributed, Bidirectional, Dense, Layer, InputSpec\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"token\"].values.tolist(),\n",
    "                  data[\"iob_label\"].values.tolist())\n",
    "    return [(token, iob_label) for token, iob_label in iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "  all_words = list(set(data[\"token\"].values))\n",
    "  all_tags = list(set(data[\"iob_label\"].values))\n",
    "\n",
    "  word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "  word2index[\"--UNKNOWN_WORD--\"] = 0\n",
    "\n",
    "  word2index[\"--PADDING--\"] = 1\n",
    "\n",
    "  index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "  tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "  tag2index[\"--PADDING--\"] = 0\n",
    "\n",
    "  index2tag = {idx: word for word, idx in tag2index.items()}\n",
    "\n",
    "  return word2index, index2word, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reports, word2index, tag2index, max_sentence_size=512):\n",
    "  contents = []\n",
    "  labels = []\n",
    "  for report in reports:\n",
    "    content = []\n",
    "    label = []\n",
    "    for i in range(len(report)):\n",
    "      token, iob_tag = report[i]\n",
    "      word_idx = word2index.get(token, 0)\n",
    "      tag_idx = tag2index.get(iob_tag, 0)\n",
    "      content.append(word_idx)\n",
    "      label.append(tag_idx)\n",
    "\n",
    "    contents.append(content)\n",
    "    labels.append(label)\n",
    "\n",
    "  contents = tf.keras.preprocessing.sequence.pad_sequences(contents, maxlen=max_sentence_size, padding='post', value=1)\n",
    "  labels = tf.keras.preprocessing.sequence.pad_sequences(labels, maxlen=max_sentence_size, padding='post')\n",
    "\n",
    "  tag_size = len(tag2index)\n",
    "\n",
    "\n",
    "  labels_categorical = [tf.keras.utils.to_categorical(i, num_classes=tag_size) for i in labels]\n",
    "  labels_categorical = np.asarray(labels_categorical)\n",
    "\n",
    "  return contents, labels, labels_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 transitions=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = transitions\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CRF, self).get_config()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'transitions': self.transitions.numpy()  # Convert the transitions to a NumPy array\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Since 'transitions' is a NumPy array, we need to convert it back to a tensor\n",
    "        transitions = tf.convert_to_tensor(config['transitions'])\n",
    "        # Create a new instance of CRF with the saved configuration\n",
    "        return cls(output_dim=config['output_dim'], sparse_target=config['sparse_target'], transitions=transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(input_dim, output_dim, input_length, mask_zero):\n",
    "    return Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = mask_zero)\n",
    "\n",
    "def bilstm_crf(maxlen, n_tags, lstm_units, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim = n_words, output_dim = embedding_dim, input_length = maxlen, mask_zero = mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Bidirectional(LSTM(units = lstm_units, return_sequences = True, recurrent_dropout = 0.1))(output)\n",
    "\n",
    "    # Dense layer\n",
    "    output = TimeDistributed(Dense(n_tags, activation = 'relu'))(output)\n",
    "\n",
    "    output = CRF(n_tags, name = 'crf_layer')(output)\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word_test_sentences_and_tags(index2tag, index2word, X_test, y_test):\n",
    "\n",
    "    test_sentences= []\n",
    "    test_tags = []\n",
    "\n",
    "    # Recupera os laudos e tags no formato word2index/tag2index\n",
    "    for i in range(len(X_test)):\n",
    "        aux_tag = []\n",
    "\n",
    "        report = \"\"\n",
    "        sentence = X_test[i]\n",
    "        tags = y_test[i]\n",
    "\n",
    "        # Recupera o laudo\n",
    "        for j in range(len(sentence)):\n",
    "            # Recupera a palavra\n",
    "            word = sentence[j]\n",
    "            # Recupera a tag\n",
    "            tag = tags[j]\n",
    "            int_tag = np.where(tag == int(1))\n",
    "            # Constrói o laudo ignorando as palavras \"padding\"\n",
    "            # Constrói o array de tags do laudo\n",
    "            if str(index2word[word]) != '--PADDING--':\n",
    "                report = report + \" \" + str(index2word[word])\n",
    "                aux_tag.append(index2tag[int(int_tag[0][0])])\n",
    "\n",
    "        test_sentences.append(report)\n",
    "        test_tags.append(aux_tag)\n",
    "\n",
    "    return test_sentences, test_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparams, n_words, n_tags, text_sequences, tag_sequences_categorical):\n",
    "    # Criar o modelo\n",
    "    model = bilstm_crf(\n",
    "        maxlen=hyperparams['max_len'], \n",
    "        n_tags=n_tags, \n",
    "        lstm_units=hyperparams['lstm_units'], \n",
    "        embedding_dim=hyperparams['embedding_dim'], \n",
    "        n_words=n_words, \n",
    "        mask_zero=True\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Compilar o modelo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hyperparams['learning_rate']), \n",
    "        loss=model.layers[-1].loss, \n",
    "        metrics=model.layers[-1].accuracy\n",
    "    )\n",
    "\n",
    "    # Definir callbacks\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, verbose=1),\n",
    "        EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Treinar o modelo\n",
    "    model.fit(\n",
    "        text_sequences, \n",
    "        tag_sequences_categorical, \n",
    "        epochs=hyperparams['max_epochs'], \n",
    "        callbacks=callbacks, \n",
    "        verbose=1, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train = pd.read_csv('../data/df_train_llms_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "data_train = pd.read_csv('../data/df_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "data_test = pd.read_csv('../data/df_test_llms_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, tag2index, index2tag = build_vocab(data_train)\n",
    "reports = data_train.groupby(\"report\").apply(to_tuples).tolist()\n",
    "text_sequences, tag_sequences, tag_sequences_categorical = tokenize(reports, word2index, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = text_sequences\n",
    "y_train = tag_sequences_categorical\n",
    "train_sentences, train_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, text_sequences, tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir hiperparâmetros\n",
    "hyperparams = {\n",
    "    'max_len': 512,\n",
    "    'lstm_dropout': 0.1,\n",
    "    'max_epochs': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'embedding_dim': 300,\n",
    "    'lstm_units': 50,\n",
    "    'batch_size': 8\n",
    "}\n",
    "\n",
    "# Configurações do modelo\n",
    "n_words = len(word2index)\n",
    "n_tags = len(tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 512)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 512, 300)          1078500   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512, 100)         140400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 512, 14)          1414      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 512, 14)           196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,220,510\n",
      "Trainable params: 1,220,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "31/31 [==============================] - 19s 472ms/step - loss: 143.9930 - viterbi_accuracy: 0.9263 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 14s 450ms/step - loss: 20.5368 - viterbi_accuracy: 0.9872 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 14s 451ms/step - loss: 11.8784 - viterbi_accuracy: 0.9928 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 14s 454ms/step - loss: 7.7326 - viterbi_accuracy: 0.9956 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 14s 457ms/step - loss: 5.4924 - viterbi_accuracy: 0.9965 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 14s 457ms/step - loss: 4.0953 - viterbi_accuracy: 0.9974 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 14s 458ms/step - loss: 3.4112 - viterbi_accuracy: 0.9977 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 14s 456ms/step - loss: 3.0600 - viterbi_accuracy: 0.9977 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 14s 455ms/step - loss: 2.6375 - viterbi_accuracy: 0.9982 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 14s 455ms/step - loss: 2.2826 - viterbi_accuracy: 0.9983 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "model = train(hyperparams, n_words, n_tags, text_sequences, tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_df_model_previous(test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE=512):\n",
    "\n",
    "    test_df = pd.DataFrame(columns = ['report', 'word', 'tag', 'tag_pred'])\n",
    "\n",
    "    for i in range (len(test_sentences)):\n",
    "\n",
    "        # Gera os laudos no formato index2word com o tamanho max_sentence\n",
    "        sentence = test_sentences[i]\n",
    "        tags = test_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        padded_sentence = sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence))\n",
    "        padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
    "\n",
    "        # Faz a predição das tags das palavras\n",
    "        pred = model.predict(np.array([padded_sentence]))\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "test_reports = data_test.groupby(\"report\").apply(to_tuples).tolist()\n",
    "test_text_sequences, test_tag_sequences, test_tag_sequences_categorical = tokenize(test_reports, word2index, tag2index)\n",
    "test_sentences, test_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, test_text_sequences, test_tag_sequences_categorical)\n",
    "result_df_model_00 = result_df_model_previous(test_sentences, test_tags, model, word2index, index2tag)\n",
    "result_df_model_00.to_csv(\"result_df_model.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilstmcrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
